{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 7000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'\n",
    "eval_iters = 200\n",
    "n_emb = 384\n",
    "\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "\n",
    "# ------------\n",
    "\n",
    "# torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "# with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "with open('harrypotter.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 1,3,4 # batch size, sequence length, embedding size\n",
    "# x = torch.randn(B,T,C)\n",
    "x = [[[1,2,3,4],[5,6,7,8],[9,10,11,12]]]\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "# model definition\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x)\n",
    "q = query(x)\n",
    "v = value(x)\n",
    "\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T)) \n",
    "wei = torch.matmul(q, k.transpose(-2,-1)) / (head_size ** 0.5)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=1)\n",
    "# print(wei.shape)\n",
    "\n",
    "out = torch.matmul(wei, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_emb, head_size, bias=False)\n",
    "\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # A buffer is used because it is not a parameter of the model, but we want it on the GPU\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        wei = torch.matmul(q, k.transpose(-2,-1)) / (C ** 0.5)\n",
    "        wei = wei.masked_fill(self.tril == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        out = torch.matmul(wei, v)\n",
    "        return out\n",
    "\n",
    "class MutliHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(head_size*n_heads, n_emb)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embd*4, n_embd), # Projection Layer back to n_embd\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MutliHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "#Creating the Batch Normalization Layer \n",
    "class BatchNorm1d: \n",
    "    def __init__(self,dim,eps=1e-5,momentum=.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.dim = dim\n",
    "        # self.weight = torch.ones(dim) #Gain\n",
    "        # self.bias = torch.zeros(dim) #Bias\n",
    "        self.weight = torch.ones(dim) #Gain\n",
    "        self.bias = torch.zeros(dim) #Bias\n",
    "    \n",
    "\n",
    "        self.params = [self.weight,self.bias]\n",
    "\n",
    "        self.training = True\n",
    "\n",
    "    def __call__(self,x):\n",
    "        #Calculate Forward Pass\n",
    "\n",
    "        #Use batch mean\n",
    "        dim = 1\n",
    "        xmean = x.mean(dim=dim, keepdim=True)\n",
    "        xvar = x.var(dim=dim, keepdim=True)\n",
    "        \n",
    "\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) #Make it a gaussian (mean 0, std 1)\n",
    "        self.out = self.weight * xhat + self.bias #Scale and shiftA\n",
    "\n",
    "        \n",
    "\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.params \n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_emb)\n",
    "        # self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "        self.blocks = nn.Sequential(*[Block(n_emb, n_head=n_head) for _ in range(n_layer)] + [nn.LayerNorm(n_emb)])\n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size) # converge back to vocab size so that we can predict the next token \n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,n_emb)\n",
    "        logits = self.lm_head(x) # (B,T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            #Crop idx to block_size\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.3569, val loss 4.3604\n",
      "step 500: train loss 2.0065, val loss 2.0951\n",
      "step 1000: train loss 1.6105, val loss 1.7863\n",
      "step 1500: train loss 1.4404, val loss 1.6360\n",
      "step 2000: train loss 1.3483, val loss 1.5717\n",
      "step 2500: train loss 1.2804, val loss 1.5277\n",
      "step 3000: train loss 1.2318, val loss 1.5033\n",
      "step 3500: train loss 1.1898, val loss 1.4870\n",
      "step 4000: train loss 1.1529, val loss 1.4832\n",
      "step 4500: train loss 1.1187, val loss 1.4784\n",
      "step 5000: train loss 1.0852, val loss 1.4832\n",
      "step 5500: train loss 1.0504, val loss 1.5027\n",
      "step 6000: train loss 1.0191, val loss 1.4962\n",
      "step 6500: train loss 0.9856, val loss 1.5165\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLA.EDWARCLAS:\n",
      "CLAUFIS:\n",
      "All, my wounds:\n",
      "Go: bo by and halloopp him back.\n",
      "\n",
      "His nobortomages:\n",
      "How now, I warrandly, will, madam: if we chare him no title,\n",
      "Beat him turning, as we should revenged on one\n",
      "Thus chase would trust have lessly.\n",
      "\n",
      "ROMEO:\n",
      "O, for my mindy! O my sovereign, my lord!\n",
      "Abo's father: I would I were here not spirit\n",
      "That; when I wis. If it condit, sirrah,\n",
      "If you do deserve your friends, here with our\n",
      "To redirest your complaining is full of sport:\n",
      "It is too commandment, too much nettless\n",
      "To maintard, you hither will chowarment.\n",
      "We may be Dester? No, n aught and Richard\n",
      "\n",
      "HORTENSIO:\n",
      "A grave, I have mine, people so much unle:\n",
      "Unto the despairing as shall havews of him\n",
      "As I to ream coming: therefore, the them\n",
      "Down in the common our formasties of you: your eyes,\n",
      "You must we do fear, and, I obear,--\n",
      "\n",
      "Nurse:\n",
      "Good Catesby, and, dispatch-enobled,\n",
      "Would but negliciously?\n",
      "\n",
      "CATESBY:\n",
      "Fave was an't; yes from the best sense this bloth,\n",
      "And beg on your tears of its,--\n",
      "Let them be by wind, bear your spicion,--\n",
      "Which are you abide in as pieced with a citche\n",
      "As an unknown gentlewoman to a hearing.\n",
      "\n",
      "Servant:\n",
      "Away, sir.\n",
      "\n",
      "First Servant:\n",
      "What's then!\n",
      "\n",
      "THOMAS Mayor:\n",
      "Whom he did! methinks ere there apt he men\n",
      "So might constenter purblic twenty craverence,\n",
      "Than for himself to her prey, whence was were?\n",
      "\n",
      "Page:\n",
      "Go with may play for matter, here have I mine,\n",
      "I shall tent means but well thy wife.\n",
      "\n",
      "GRUMIO:\n",
      "'Tis very soul, that has been made, if good worse:\n",
      "Pratiful have been wit thee, whose view take love;\n",
      "And then in the farmer of the world\n",
      "Break hath hearts off: when I did lie at says\n",
      "She prompt thee! First, why late?\n",
      "Or else he encount her would so occlaim'd,\n",
      "With clouder o'er private lister breeds.\n",
      "O curse him beaut hut me, hills,\n",
      "The which cheer to o'er-dent himself!\n",
      "For father well thine, the were disposed\n",
      "With great happy ear, to ourself.\n",
      "\n",
      "DERMEO:\n",
      "On the gates: good with remore comes I\n",
      "I hell tarry, and will entertained to his;\n",
      "Patrements here and the lads have something in rege\n",
      "The better manhosters of me all now\n",
      "in places to both it: by lifiences there,\n",
      "If it be ruther'd everselves.\n",
      "Would you do for than effect thee I broan,\n",
      "That more by the friar I hate through were,\n",
      "Than whether I comely thee on thy son,\n",
      "Thine is the fixed of hard my bewray,\n",
      "Thou werthy all'st underchless in thy best\n",
      "Thy brother's blood and burialding doth rich\n",
      "And say to be her sisport like the depute:\n",
      "Have too grief, confining a babe,\n",
      "To trry 'Who that my dear and would touch'd\n",
      "Should within my persuasing post!\n",
      "\n",
      "WESTMONES:\n",
      "'Tis like shall tent:\n",
      "And, Blesces well, greet King Hereford,\n",
      "Excepting beginning, and I this.\n",
      "\n",
      "BALTHASAR:\n",
      "Well, my good lord, they say have many well:\n",
      "So must I live unto do what object\n",
      "Your grace in here want down to one less,\n",
      "Thy vow many married morner, disdain to renders\n",
      "I'll stay adverse.\n",
      "\n",
      "CORIOLANUS:\n",
      "I have town: I have forgot to it.\n",
      "For you know not, sir, sir, to you\n",
      "It upwearing with silver and to him there:\n",
      "You can receive\n",
      "Where is not harden of sorrow camest or harms,\n",
      "We would kill ve this offender's descaution!\n",
      "\n",
      "Lord Mayor:\n",
      "The better that not the Capitol-hails of half,\n",
      "So from use hard, Clarence, we will smarried\n",
      "Then 'tis in a married chief with 'er strives.'\n",
      "Why, leaving up thy time, for I find him stay:\n",
      "Thereforetorto, for wallible.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "Here is no tongue may not be wating. Go to heaven,\n",
      "I wonder'd mother 'longs, in flowers' mornion,\n",
      "Than 'twas not by chience, by your put in ransoln,\n",
      "Your dancing martality\n",
      "Fire as undoing when I left the two\n",
      "To see shine. Why, then I would not rest not\n",
      "To the sun, further.\n",
      "\n",
      "CORIOLANUS:\n",
      "Why, one sir! and my poor Juphant,\n",
      "Gracious queen and revenges as bold good\n",
      "Murder the father's; for this oney\n",
      "Which, though not our charity: you'll under\n",
      "'Twill you and not wake. Will't do?\n",
      "Thesey but the time would lay and the would do?\n",
      "\n",
      "Messenger:\n",
      "His brother, my Lord Duke of Hereford!\n",
      "\n",
      "LEONTES:\n",
      "Your sword, sirs, thou art worth: let us kill'd.\n",
      "\n",
      "LEONTES:\n",
      "Sir, you so.\n",
      "\n",
      "LEONTES:\n",
      "Wha'? my gentle Paulina, 'twas I an whisper thing they\n",
      "you may achorr'd, for far you tell us.\n",
      "\n",
      "PLIFFORIZABETH:\n",
      "Sir, that London Rome.\n",
      "\n",
      "LUCIO:\n",
      "\n",
      "LUCENTES:\n",
      "That have I done opposed to chide.\n",
      "\n",
      "CAMILLO:\n",
      "Even in this true! Wend why shall what I do?\n",
      "\n",
      "POLIXENES:\n",
      "What, marry, whilst?\n",
      "\n",
      "POLIXENES:\n",
      "A gentleman crams!\n",
      "\n",
      "VALERIA:\n",
      "She hath, the dangerone tribunes. Why have\n",
      "your dear none to fair?P\n",
      "\n",
      "PERDITA:\n",
      "So ville will I: pause hereto reply.\n",
      "\n",
      "POLIXENES:\n",
      "The prite,\n",
      "Farewell.\n",
      "\n",
      "MARIZABELLA:\n",
      "Hoo! wrongly says was and warlion?\n",
      "\n",
      "CLAUDIO:\n",
      "But never and brought you, for your wife,\n",
      "That avain. Lend if your head, I am true!\n",
      "And you see, or libery wisdoms,\n",
      "Yould bring farther on the late-clock, and be wreap'd,\n",
      "Want that then to saved, so say, with ten things\n",
      "More but obes, which as theu art gost,\n",
      "By flint: boind with the testanders heir proofs--\n",
      "As is God, if and swains on the last;\n",
      "O, and when my first, were I land to know it,\n",
      "Of his brother's death; for I come all move:\n",
      "Yet shall thee were it great to thy advice\n",
      "As his benefit death\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 256), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=5000)[0].tolist())[256:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.788929 M parameters\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
