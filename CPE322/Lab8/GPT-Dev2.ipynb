{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'\n",
    "eval_iters = 200\n",
    "n_emb = 32\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = get_batch('train')\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 1,3,4 # batch size, sequence length, embedding size\n",
    "# x = torch.randn(B,T,C)\n",
    "x = [[[1,2,3,4],[5,6,7,8],[9,10,11,12]]]\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "# model definition\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x)\n",
    "q = query(x)\n",
    "v = value(x)\n",
    "\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T)) \n",
    "wei = torch.matmul(q, k.transpose(-2,-1)) / (head_size ** 0.5)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=1)\n",
    "# print(wei.shape)\n",
    "\n",
    "out = torch.matmul(wei, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self,head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_emb, head_size)\n",
    "        self.query = nn.Linear(n_emb, head_size)\n",
    "        self.value = nn.Linear(n_emb, head_size)\n",
    "\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # A buffer is used because it is not a parameter of the model, but we want it on the GPU\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        wei = torch.matmul(q, k.transpose(-2,-1)) / (C ** 0.5)\n",
    "        wei = wei.masked_fill(self.tril == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=1)\n",
    "\n",
    "        out = torch.matmul(wei, v)\n",
    "        return out\n",
    "\n",
    "class MutliHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        return out\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_emb)\n",
    "        # self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "        self.sa_heads = MutliHeadAttention(4, n_emb//4)\n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size) # converge back to vocab size so that we can predict the next token \n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.sa_heads(x) # (B,T,n_emb)\n",
    "        logits = self.lm_head(x) # (B,T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            #Crop idx to block_size\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.1682, val loss 4.1727\n",
      "step 300: train loss 1.3057, val loss 1.2969\n",
      "step 600: train loss 0.9667, val loss 1.0017\n",
      "step 900: train loss 0.8189, val loss 0.8479\n",
      "step 1200: train loss 0.7638, val loss 0.7973\n",
      "step 1500: train loss 0.7025, val loss 0.7066\n",
      "step 1800: train loss 0.6710, val loss 0.7045\n",
      "step 2100: train loss 0.6654, val loss 0.6956\n",
      "step 2400: train loss 0.5975, val loss 0.6304\n",
      "step 2700: train loss 0.5705, val loss 0.6044\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bove is oodwer yourastheors sort, I lis be heseghoulcithe oto scitith the is that ls che bot ot ot ete do he whake odat mor noto these sou thes isger tohe the lirsth che in thate shou oke sh uptoke ando sts th of ast mobe Iose a yount kat od wha loode kse the jor bolad\n",
      "That gst ias otth secod pod thaperok th thousp ato sh otheaghor th dicols whe lokm ss theo Is hem huspot ket thust the I the st 'sd tht yousciod do acheg kes, gott ordose whe 'swh the pok poufer mas\n",
      "od he dithough the ! lomanete fs he andat po bithe reros chend as pousth the ghound owh cars gh wake ad weo lotthad Hough ot the potent he be st the ard thel rsuroxne rt tham whake agh mand'egenso of bet the etther mon ache mo soll he\n",
      "Ars ood th pat qu,\n",
      "STARDY loke morod blokter odefe rote rgerou che othurtessod fa ss se ar datho che whand che poto sher oo'ught th thentito pihe noo theteghoulgeo od whastakis oto 'rge fobe cot tak sthorove this om th geg. Wher oth't IcChatis o glor shey;\n",
      "Tod omates th theanwhe otot aud ootth ther som beis pous ok tok the ast VIORTUTHitrth msast ugh th I VIOf lok do somoroke ee qur sh the so xhe whe ante othe ast'i, VIOKSinot he the orsod whe heshou the notece VIFICINIUTISINIUSBREHTow thoms oures ot she roves'et que angotthef e ehm wour a lo ther rg od pof be othe I roghountst spoodecheenteo s od the exbe pot sh shome ng ote mo un exlt rgth theve the otthat Hillok gne bor sthat the thehe ar rowed potth pome cheo sh otthe aist the I LARDUKINIUSBRANTARDY he thet, theosnotthe he oke use then; he brod cher the bo wentithe thak with he wome I the r so VISTAs he oome rthe ing that ook ood wiso glosmass he I an wha scotent od he doo Hingerangod hat kateght ingoterokmiset a he lher mupgeris noke he an,\n",
      "ADYRY othat ot and the ostake th che eche loothake hour yougho ss noto wiepouken whate sont, loot gth frokl is yuthe od po the'lviet asthe had sh loreshat othes od the VISGLTbothese thald sh the othoughouds otthee es gho whe lhe ke\n",
      "Vwhe bloke hen; aeve she notsme ssein'to theakesth he is Lous cont os iloht thenod cotho grtithe lioted,\n",
      "Swe brod ke arcotstikno the If oot woun the othesmingto rst s od sh be lohoust oetet the the loktt qu, isth islihen et odtthe mouse nott noth som, sh thasoull wh lok pot I no'loupre the loom to tisr al I yourgeo of gas rsth thofe I th hino ot he whe thol sg lo ode whe sp sord 'se sg che rs sst ot stakepightoodmaste IdCthe the sod Thor et ke tore unt\n",
      "he at los\n",
      "od mof ar okepis\n",
      "ods that podt mo th the th and, esom mont so otekeve th woe oo th shatlonget\n",
      "HMif do dood, qu; os.\n",
      "\n",
      "LARDWYow I yake omat de he ok ods prot o che rsthed the soudto et these dallilger s rgegep thesr thewer ec and tarm andd th osnotedis ist thed rge'a whitht fhist th gr ate ak\n",
      "Twe is hat hand tharloto elsouscoud;\n",
      "Soef he notth isces Bovei lo th mt, se stse'i, es cheroche srod ome asersoult oe\n",
      "owherthis la th ast othouthe heo sh oke yore is I wixtere he et\n",
      "Tome pou fs pome hou som aul batthsure looke asthe th monketis otchersear do the the fre as ars thes at jou soot to sount,\n",
      "ARD jore ther the st the jorook r lhe sthe OSISTEThat lot assodo' sound th lis loy loooke lokeslod mad aloke rd che wh\n",
      "'TARDY dorot then;\n",
      "The oke her dose he whisenothe dst th will sh qu,\n",
      "X'Rof reverse th ood 'lBood har pat caf paket a theshou dokt am ho the as lok sh thtohf st he whou he gh ant he thesount othade paken:\n",
      "Sow XVIOn so doltake notorgisheake he hat he rgokes your os ye re oghoms the cokt her no somet vrinket whe thent, isse pough lad gote the ustho nod I lhe ancod dos coed ootche ther thof theceten'th oth code is somme shou th ise lo IIFIr sood thaketh whe is mpa wipow ss' winoth lonea not he be nottoll the\n",
      "Those shake so what 'l iss to gatorso soul wed,--at inet the risge al ighoeve rentoker lod ouvetilomp ode athous IDY noke-nt theoote of qu,-m\n",
      "Bover whet to glort to dorl lsotsts the ot th the asth s! 'lvito' fo her shist thatett whe lo'urs and sother th the od, od ca; airthe to th aspotthe chout lo inoth thorst thes thasto ss he she\n",
      "Thaketihe allok sone\n",
      "t\n",
      "To whe Sous\n",
      "\n",
      "Gok mast bous thathalk.\n",
      "\n",
      "KILLREWBREY odo wims sok lokths sod 'se\n",
      "Bot thingok the be; had patithemakepithe'rotm won thtakeved;\n",
      "Amlispods od et andosen; gediche r got the lo esh theo mo ust rse re sus buthe ood\n",
      "he the mo sh the he I not ing ortiottheo odt, odt nge noto the es' the forgeas bo mo omye aingods the ot he od the ast odtepod she is th eke ch decc oth st heo the ot th, sot ankessom woe sh Ro3 otto he ood pat heghoe her andtortis ott od whe rsksee ISLARDY that cheis theiroche che rd port god be lod thouthd ss odd orthmal the whe somet ke as wought lood qu, nothe pong I od ath keve Id boatlo powe not ot cod hamd J scere ss he mant then; the st looderot sh the ghourvif noke sh lok et 'so s che node pot dl'rtit eg?\n",
      "No the the nithomounkererot scat dos-hmake loke pou pos th dokthash ooo whe do om the od ackm do wolllaevedive Csow fs os love somy f r hof le thegs some nok seke : wh rot take es th at ton eghough es lotthat arthe otme s r do'd qu, isst s I mtle, rso wen; ar \n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 8), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=5000)[0].tolist())[8:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
