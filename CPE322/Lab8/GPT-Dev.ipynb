{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the dataset\n",
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Input Text\n",
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "#Get All Unique Characters\n",
    "chars = sorted(list(set(text)))\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 2]\n",
      "Hello!\n"
     ]
    }
   ],
   "source": [
    "#Tokenize the text- This will be used to convert the text to numbers\n",
    "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
    "idx_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "#Encode and Decode Functions for Strings\n",
    "def encode(s):\n",
    "    return [char_to_idx[ch] for ch in s]\n",
    "def decode(l):\n",
    "    return ''.join([idx_to_char[i] for i in l])\n",
    "\n",
    "#Test\n",
    "encodedText = encode(\"Hello!\")\n",
    "print(encodedText)\n",
    "print(decode(encodedText))\n",
    "\n",
    "#Note: This is a very simple tokenization method. There are better ways to do this. Example: Using tiktoken from OpenAI. (Sub-word tokenization = Don't need a new token for every letter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2f822015010>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Some Imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of encoded text:  1115394\n",
      "torch.Size([1115394])\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n"
     ]
    }
   ],
   "source": [
    "#Encode the input text and store it into a Torch Tensor\n",
    "#A tensor is a multi-dimensional matrix containing elements of a single data type\n",
    "\n",
    "#Convert the text to numbers\n",
    "encodedText = encode(text)\n",
    "print(\"Length of encoded text: \", len(encodedText))\n",
    "\n",
    "#Convert the list to a tensor\n",
    "data = torch.tensor(encodedText, dtype=torch.long)\n",
    "print(data.shape)\n",
    "print(data[:10]) #First 10 elements (First 10 characters in the text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of data:  1115394\n",
      "Length of training data:  1003854\n",
      "Length of validation data:  111540\n"
     ]
    }
   ],
   "source": [
    "#Get Training and Validation Data\n",
    "#We will use the first 90% of the data for training and the last 10% for validation\n",
    "\n",
    "#Get the length of the data\n",
    "dataLen = len(data)\n",
    "print(\"Length of data: \", dataLen)\n",
    "\n",
    "#Get the length of the training data  \n",
    "trainLen = int(dataLen * 0.9)\n",
    "\n",
    "#Get the training data\n",
    "train_data = data[:trainLen]\n",
    "print(\"Length of training data: \", len(train_data))\n",
    "\n",
    "#Get the validation data\n",
    "val_data = data[trainLen:]  \n",
    "print(\"Length of validation data: \", len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence:  tensor([18, 47, 56, 57, 58,  1, 15, 47])\n",
      "Output sequence:  tensor([47, 56, 57, 58,  1, 15, 47, 58])\n",
      "1 : Context:  tensor([18])  Target:  tensor(47)\n",
      "2 : Context:  tensor([18, 47])  Target:  tensor(56)\n",
      "3 : Context:  tensor([18, 47, 56])  Target:  tensor(57)\n",
      "4 : Context:  tensor([18, 47, 56, 57])  Target:  tensor(58)\n",
      "5 : Context:  tensor([18, 47, 56, 57, 58])  Target:  tensor(1)\n",
      "6 : Context:  tensor([18, 47, 56, 57, 58,  1])  Target:  tensor(15)\n",
      "7 : Context:  tensor([18, 47, 56, 57, 58,  1, 15])  Target:  tensor(47)\n",
      "8 : Context:  tensor([18, 47, 56, 57, 58,  1, 15, 47])  Target:  tensor(58)\n"
     ]
    }
   ],
   "source": [
    "block_size = 8 #The length of the sequence we want to train on (the context)\n",
    "#Example: If block_size = 8, then we want to predict the 9th character in the sequence\n",
    "#This is why we will need to train the model on sequences of length block_size + 1\n",
    "#In the below code the max value of t+1 is block_size\n",
    "\n",
    "x = train_data[0:block_size] # The input sequence\n",
    "y = train_data[1:block_size+1] # The output sequence\n",
    "\n",
    "print(\"Input sequence: \", x)\n",
    "print(\"Output sequence: \", y)\n",
    "\n",
    "for t in range(block_size): \n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(t+1, ': Context: ', str(context), ' Target: ', str(target)) \n",
    "    #Ex. If data is {1,2,3} and we are given 1, then the target (predicted output) should be 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape:  torch.Size([4, 8])\n",
      "Output batch shape:  torch.Size([4, 8])\n",
      "-----------------\n",
      "Input batch: \n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "-----------------\n",
      "Output batch: \n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "-----------------\n",
      "1 : Context:  tensor([[24],\n",
      "        [44],\n",
      "        [52],\n",
      "        [25]])  \n",
      "Target:  tensor([43, 53, 58, 17])\n",
      "2 : Context:  tensor([[24, 43],\n",
      "        [44, 53],\n",
      "        [52, 58],\n",
      "        [25, 17]])  \n",
      "Target:  tensor([58, 56,  1, 27])\n",
      "3 : Context:  tensor([[24, 43, 58],\n",
      "        [44, 53, 56],\n",
      "        [52, 58,  1],\n",
      "        [25, 17, 27]])  \n",
      "Target:  tensor([ 5,  1, 58, 10])\n",
      "4 : Context:  tensor([[24, 43, 58,  5],\n",
      "        [44, 53, 56,  1],\n",
      "        [52, 58,  1, 58],\n",
      "        [25, 17, 27, 10]])  \n",
      "Target:  tensor([57, 58, 46,  0])\n",
      "5 : Context:  tensor([[24, 43, 58,  5, 57],\n",
      "        [44, 53, 56,  1, 58],\n",
      "        [52, 58,  1, 58, 46],\n",
      "        [25, 17, 27, 10,  0]])  \n",
      "Target:  tensor([ 1, 46, 39, 21])\n",
      "6 : Context:  tensor([[24, 43, 58,  5, 57,  1],\n",
      "        [44, 53, 56,  1, 58, 46],\n",
      "        [52, 58,  1, 58, 46, 39],\n",
      "        [25, 17, 27, 10,  0, 21]])  \n",
      "Target:  tensor([46, 39, 58,  1])\n",
      "7 : Context:  tensor([[24, 43, 58,  5, 57,  1, 46],\n",
      "        [44, 53, 56,  1, 58, 46, 39],\n",
      "        [52, 58,  1, 58, 46, 39, 58],\n",
      "        [25, 17, 27, 10,  0, 21,  1]])  \n",
      "Target:  tensor([43, 58,  1, 54])\n",
      "8 : Context:  tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])  \n",
      "Target:  tensor([39,  1, 46, 39])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337) #Set the seed for reproducibility\n",
    "block_size = 8 #The length of the sequence we want to train on \n",
    "batch_size = 4 #The number of sequences we want to train on at a time\n",
    "\n",
    "def get_batch(split):\n",
    "    #Get the data depending on the split (train or validation)\n",
    "    if split == 'train':\n",
    "        data = train_data\n",
    "    else:\n",
    "        data = val_data\n",
    "    \n",
    "    #Get batch_size random starting indexes\n",
    "    ix = torch.randint(high=len(data)-block_size, size=(batch_size,))\n",
    "\n",
    "    #Construct the input and output sequences\n",
    "    x = [data[i:i+block_size] for i in ix]\n",
    "    y = [data[i+1:i+block_size+1] for i in ix]\n",
    "    \n",
    "    #Convert the sequences to tensors of size (batch_size, block_size)\n",
    "    x = torch.stack(x)\n",
    "    y = torch.stack(y)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "#Test\n",
    "xb, yb = get_batch('train')\n",
    "print(\"Input batch shape: \", xb.shape)\n",
    "print(\"Output batch shape: \", yb.shape)\n",
    "\n",
    "print(\"-----------------\")\n",
    "print(\"Input batch: \")\n",
    "print(xb)\n",
    "print(\"-----------------\")\n",
    "print(\"Output batch: \")\n",
    "print(yb)\n",
    "print(\"-----------------\")\n",
    "\n",
    "for t in range(block_size): #Basically the same as the previous but now we are using batches (multiple sequences)\n",
    "    context = xb[:, :t+1] #Get the first t+1 elements of each sequence\n",
    "    target = yb[:, t]\n",
    "    print(t+1, ': Context: ', str(context), ' \\nTarget: ', str(target)) \n",
    "    #Ex. If data is {1,2,3} and we are given 1, then the target (predicted output) should be 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "\n",
      "bL?DP-QWkrEoL?jLDJQcLVf'RIHD'Hdhs Yv,ixa,jswYZwLEuS'paokqOzs$!A$zFGQT;eMk x.gQ$FCLg!iWn.O!zDGyA YsT3\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        #super() returns the parent object of the class\n",
    "        #In this case, it returns the parent object of BigramLanguageModel which is nn.Module (defined above)\n",
    "        super().__init__() # Call the parent class constructor\n",
    "\n",
    "        \n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) #Embedding layer\n",
    "        #Embedding layer is used to convert the input tokens to vectors of size embedding_dim\n",
    "        #In this case, the input tokens are numbers (the encoded text) and the embedding_dim is vocab_size\n",
    "        #Vectors will allow us to do math operations on the tokens\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        logits = self.token_embedding_table(idx)*.1 #Get the embeddings for the input tokens. (Batch, Time, Channel) or (Batch, Block_size, vocab_size)\n",
    "\n",
    "        if targets != None:\n",
    "            logits = logits.view(-1, logits.shape[-1]) \n",
    "            print(logits.shape)\n",
    "            loss = F.cross_entropy(logits, targets.view(-1)) #Calculate the loss. Logits and targets need to be reshaped to (Batch * Block_size, vocab_size) and (Batch * Block_size) respectively\n",
    "        else:\n",
    "            loss = None\n",
    "        \n",
    "\n",
    "\n",
    "        return logits,loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits,loss = self(idx[:,-1])\n",
    "\n",
    "            # logits,loss = self(idx)\n",
    "            # probs = F.softmax((logits[:, -1]), dim=-1)\n",
    "            probs = F.softmax((logits), dim=-1)\n",
    "            new_token = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, new_token), dim=-1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(len(chars)) #Create the model\n",
    "out,loss = m(xb, yb) #Get the output\n",
    "\n",
    "#Test Generate\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "out = ((m.generate(idx, 100).squeeze().tolist()))\n",
    "print(decode(out))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a pytorch optimizer\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0 , Loss:  2.6501591205596924\n",
      "Step:  1000 , Loss:  2.6225929260253906\n",
      "Step:  2000 , Loss:  2.5740420818328857\n",
      "Step:  3000 , Loss:  2.6518807411193848\n",
      "Step:  4000 , Loss:  2.53884220123291\n",
      "Step:  5000 , Loss:  2.667741298675537\n",
      "Step:  6000 , Loss:  2.56286358833313\n",
      "Step:  7000 , Loss:  2.527754306793213\n",
      "Step:  8000 , Loss:  2.60963773727417\n",
      "Step:  9000 , Loss:  2.5388073921203613\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000): \n",
    "    #Get the input an0d output sequences\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    #Get the logits and loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    \n",
    "    #Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #Backpropagate the loss\n",
    "    loss.backward()\n",
    "    \n",
    "    #Update the parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    #Print the loss every 10 steps\n",
    "    if steps % 1000 == 0:\n",
    "        print(\"Step: \", steps, \", Loss: \", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Owhowee:\n",
      "\n",
      " chatwaishovzKBobrem,Pr\n",
      "ymqPamoure:haisoue cear ve\n",
      "vo llillil'Tblmead mpYvJall,ony btlwd w\n"
     ]
    }
   ],
   "source": [
    "#Test Generate\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "out = ((m.generate(idx, 100).squeeze().tolist()))\n",
    "print(decode(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_emb = 32\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_emb)\n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T, vocab_size)\n",
    "\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.4908, val loss 2.5071\n",
      "step 300: train loss 2.4943, val loss 2.5115\n",
      "step 600: train loss 2.4804, val loss 2.5053\n",
      "step 900: train loss 2.4814, val loss 2.5106\n",
      "step 1200: train loss 2.4815, val loss 2.5262\n",
      "step 1500: train loss 2.4787, val loss 2.5169\n",
      "step 1800: train loss 2.4758, val loss 2.5021\n",
      "step 2100: train loss 2.4788, val loss 2.5090\n",
      "step 2400: train loss 2.4903, val loss 2.5171\n",
      "step 2700: train loss 2.4828, val loss 2.5025\n",
      "\n",
      "Men pand, bemary.\n",
      "Yor 'sour menm sora an hie.\n",
      "OLond betwe me matand thot suld me th llety ome.\n",
      "I muco ffepyotssththas l.\n",
      "TAn.\n",
      "Momo thapeinthesse ed Pe bene ovetour?\n",
      "Cassches os cok hedin tie s indo hisor fe f tas :\n",
      "Whit Cl s;\n",
      "\n",
      "Tundisthou ldu he n,\n",
      "Fo mone.\n",
      "\n",
      "Anthatakes agherchin\n",
      "NENam s s withoumas Fond thacolld INour id,\n",
      "Tcks honourd?\n",
      "TI idurd po veno nond Ce?\n",
      "Fy\n",
      "Se thisoou tiund tho nofen e sutan wiporthare whanothavitthers,\n",
      "\n",
      "O, Bllollke, on s h O, t pan, cr wat hotive wout ir f; u;\n",
      "\n",
      "Fe ce inee\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4, 8, 2 # batch size, context length, vocab size\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's make the context talk to each other\n",
    "# Essentially, the value for a specific context will be the average of the values of the context that came before it.\n",
    "# Ex. If the context is \"Matthe\" and the next character is \"w\", then the value for \"w\" will be the average of the values for \"M, a, t, t, h, e, w\" \n",
    "\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for batch in range(B):\n",
    "    for context in range(T):\n",
    "        xprev = x[batch, :context+1]\n",
    "        xbow[batch, context] = xprev.mean(dim=0)\n",
    "\n",
    "# Or alternatively \n",
    "\n",
    "wei = torch.tril(torch.ones((T,T)))\n",
    "wei = wei / wei.sum(dim=1, keepdim=True)\n",
    "xbow2 = wei @ x\n",
    "\n",
    "torch.allclose(xbow, xbow2) \n",
    "\n",
    "\n",
    "# Or alternatively\n",
    "tril = torch.tril(torch.ones((T,T)))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril==0, float('-inf')) # Make the 0s -inf\n",
    "wei = F.softmax(wei, dim=1) # Will equal wei from the prev part (Because softmax will exponentiate the -inf to 0 and 0s to 1 and then normalize)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = torch.zeros((2,3))\n",
    "display(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
