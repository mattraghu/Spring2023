{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load up names.txt\n",
    "words = open('names.txt', 'r').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a list of all the chars\n",
    "chars = ['.'] + sorted(list(set(''.join(words))))\n",
    "\n",
    "#Tokenize the text- This will be used to convert the text to numbers\n",
    "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
    "idx_to_char = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 182625 Val: 22655 Test: 22866\n"
     ]
    }
   ],
   "source": [
    "#Build data set\n",
    "vocab_size = len(chars)\n",
    "block_size = 3\n",
    "\n",
    "def build_data_set(words):\n",
    "    #Get the inputs and targets\n",
    "    X, Y = [], []\n",
    "\n",
    "    for word in words[:]:\n",
    "        word = \".\" + word + \".\"\n",
    "        context = [0] * block_size\n",
    "        for i in range(len(word)-1):\n",
    "            chr = char_to_idx[word[i]]\n",
    "            context.append(chr)\n",
    "            if len(context) > block_size:\n",
    "                context = context[1:]\n",
    "            X.append(context[:])\n",
    "            Y.append(char_to_idx[word[i+1]])\n",
    "\n",
    "            # print(\"For context\", context, \"predict\", word[i+1])\n",
    "\n",
    "    #Convert to tensor\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "#Shuffle the data\n",
    "random.shuffle(words)\n",
    "\n",
    "n1 = int(len(words)*0.8)\n",
    "n2 = int(len(words)*0.9)\n",
    "\n",
    "X_train, Y_train = build_data_set(words[:n1])\n",
    "X, Y = build_data_set(words[:n1]) #Just for prev code\n",
    "X_val, Y_val = build_data_set(words[n1:n2])\n",
    "X_test, Y_test = build_data_set(words[n2:])\n",
    "print(\"Train:\", len(X_train), \"Val:\", len(X_val), \"Test:\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Linear Layer Class\n",
    "class Linear:\n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        self.weight = torch.randn(fan_in, fan_out) / fan_in**.5\n",
    "        self.bias = torch.zeros(fan_out) if bias else None\n",
    "        self.params = [self.weight] + ([self.bias] if bias else [])\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight \n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.params\n",
    "\n",
    "#Create a Linear Layer Class to test\n",
    "linear = Linear(10, 200, False)\n",
    "\n",
    "x = torch.randn(1000,10)\n",
    "y = linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the Batch Normalization Layer \n",
    "class BatchNorm1d: \n",
    "    def __init__(self,dim,eps=1e-5,momentum=.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.dim = dim\n",
    "        # self.weight = torch.ones(dim) #Gain\n",
    "        # self.bias = torch.zeros(dim) #Bias\n",
    "        self.weight = torch.ones(dim) #Gain\n",
    "        self.bias = torch.zeros(dim) #Bias\n",
    "    \n",
    "\n",
    "        self.params = [self.weight,self.bias]\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "\n",
    "        self.training = True\n",
    "\n",
    "    def __call__(self,x):\n",
    "        #Calculate Forward Pass\n",
    "        if self.training:\n",
    "            #Use batch mean\n",
    "            xmean = x.mean(dim=0, keepdim=True)\n",
    "            xvar = x.var(dim=0, keepdim=True)\n",
    "        else: \n",
    "            #Use running mean\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) #Make it a gaussian (mean 0, std 1)\n",
    "        self.out = self.weight * xhat + self.bias #Scale and shiftA\n",
    "\n",
    "        #Update running mean and variance\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = self.momentum * xmean + (1 - self.momentum) * self.running_mean\n",
    "                self.running_var = self.momentum * xvar + (1 - self.momentum) * self.running_var\n",
    "\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = x.tanh()\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self): \n",
    "        return self.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the model to use the new layers\n",
    "\n",
    "n_embd = 10 \n",
    "n_hidden=  100\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "C = torch.randn(len(chars), n_embd, generator=g)\n",
    "\n",
    "layers = [\n",
    "    Linear(n_embd * block_size, n_hidden, bias=False), #Remembers block size is the amount of context we are using\n",
    "    BatchNorm1d(n_hidden),\n",
    "    Tanh(),\n",
    "\n",
    "    Linear(n_hidden, n_hidden, False),\n",
    "    BatchNorm1d(n_hidden),\n",
    "    Tanh(),\n",
    "\n",
    "    Linear(n_hidden, n_hidden, False),\n",
    "    BatchNorm1d(n_hidden),\n",
    "    Tanh(),\n",
    "\n",
    "    Linear(n_hidden, n_hidden, False),\n",
    "    BatchNorm1d(n_hidden),\n",
    "    Tanh(),\n",
    "    \n",
    "    Linear(n_hidden, len(chars)),\n",
    "    BatchNorm1d(len(chars)),\n",
    "]\n",
    "\n",
    "#Change the initialization of the weights like we did before \n",
    "with torch.no_grad():\n",
    "    #Last Layer should be initialized close to 0\n",
    "    layers[-1].weight *= 0.1\n",
    "    layers[-1].bias.zero_()\n",
    "    # layers[-1].gamma *= 0.1\n",
    "    # layers[-1].beta.zero_()\n",
    "\n",
    "    #Apply Kaiming Initialization to all other layers\n",
    "    for l in layers[:-1]:\n",
    "        if isinstance(l, Linear):\n",
    "            l.weight *= 5/3\n",
    "    \n",
    "#Get all the parameters of the model\n",
    "parameters = [C] + [p for l in layers for p in l.parameters()]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:     0, loss: 3.3245\n",
      "step:   100, loss: 3.2432\n",
      "step:   200, loss: 3.2111\n",
      "step:   300, loss: 3.1571\n",
      "step:   400, loss: 3.1344\n",
      "step:   500, loss: 3.0778\n",
      "step:   600, loss: 2.8708\n",
      "step:   700, loss: 3.0649\n",
      "step:   800, loss: 2.9772\n",
      "step:   900, loss: 2.8897\n",
      "step:  1000, loss: 2.9004\n",
      "step:  1100, loss: 2.8697\n",
      "step:  1200, loss: 2.8629\n",
      "step:  1300, loss: 2.9373\n",
      "step:  1400, loss: 2.8956\n",
      "step:  1500, loss: 2.7804\n",
      "step:  1600, loss: 2.7278\n",
      "step:  1700, loss: 2.7658\n",
      "step:  1800, loss: 2.8000\n",
      "step:  1900, loss: 2.7427\n"
     ]
    }
   ],
   "source": [
    "max_steps = 2000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "ud = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    #Get the minibatch\n",
    "    ix = torch.randint(len(X_train), (batch_size,), generator=g)\n",
    "    x, y = X_train[ix], Y_train[ix]\n",
    "\n",
    "    #Forward Pass\n",
    "    #First embed the characters\n",
    "    emb = C[x]\n",
    "    x = emb.view(len(x), -1) #len(X_train) x (block_size*2)\n",
    "\n",
    "    #Then pass through the layers\n",
    "    for l in layers:\n",
    "        x = l(x)\n",
    "    loss = F.cross_entropy(x, y)\n",
    "\n",
    "\n",
    "    #Backward Pass\n",
    "    for l in layers:\n",
    "        l.out.retain_grad() #Tell pytorch to keep the gradient of the output of the layer so we can use it later in our debugging\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    lr = .01 if i < 150E3 else .01\n",
    "    # lr = 1e-3\n",
    "    for p in parameters:\n",
    "        p.data -= lr * p.grad\n",
    "\n",
    "    #Track stats every once in a while\n",
    "    if i % 100 == 0:\n",
    "        print(f'step: {i:5d}, loss: {loss.item():0.4f}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters]) #Track the ratio of the gradient norm to the parameter norm\n",
    "\n",
    "    \n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "    # break\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
